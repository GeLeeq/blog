---
layout: post
title:  Hadoop I/O
date:   2016-4-27 09:43:27
comment: true
category: "Hadoop"
---

Hadoop自带一套原子操作用于数据的I/O操作。其中一些技术比Hadoop本身更常用。

### 数据完整性 ###

- hdfs的数据完整性

	HDFS会对写入的所有数据计算校验和，并在读取数据时验证校验和。每个由io.bytes.per.checksum定义的文件块大小计算校验和。（CRC-32 循环冗余校验 32bit校验结果）  
	//TODO

- LocalFileSystem和ChecksumFileSystem

	[http://blog.csdn.net/shirdrn/article/details/4574402](http://blog.csdn.net/shirdrn/article/details/4574402)

### 压缩 ###

![](http://geleeq.github.io/blog/post_res/images/hadoop/compress.jpg)

`CompressionCodec` 接口  代表一种压缩-解压缩算法。

![](http://geleeq.github.io/blog/post_res/images/hadoop/compresscodec.jpg)

>github: hadoop-lzo 它是GPL许可的，没有包含在Apache Hadoop发行版本中
	
	 //通过CompressionCodec对数据流进行压缩和解压缩   StreamCompressor.java
	 public static void main(String[] args) throws Exception {
	    String codecClassname = args[0];
	    Class<?> codecClass = Class.forName(codecClassname);
	    Configuration conf = new Configuration();
	    CompressionCodec codec = (CompressionCodec)
	      ReflectionUtils.newInstance(codecClass, conf);
	    
	    CompressionOutputStream out = codec.createOutputStream(System.out);
	    IOUtils.copyBytes(System.in, out, 4096, false);
	    out.finish();
	 }
		
	$ echo "Text" | hadoop StreamCompressor org.apache.hadoop.io.compress.GzipCodec | gunzip

	 //通过CompressionCodecFactory推断CompressionCodec
	 CompressionCodecFactory factory = new CompressionCodecFactory(conf);
     CompressionCodec codec = factory.getCodec(inputPath); // file.gz
      
	 in = codec.createInputStream(fs.open(inputPath));//很关键，读取解压后的数据
	 out = fs.create(new Path(outputUri));
     IOUtils.copyBytes(in, out, conf);

Native实现的压缩解压缩速度将会更快。 hadoop.native.lib属性可设置false关闭true打开。  

CodecPool： 反复使用压缩解压缩(Compressor对象实例），减少创建对象的开销。

	try {
      compressor = CodecPool.getCompressor(codec);
      CompressionOutputStream out =
        codec.createOutputStream(System.out, compressor); //重载方法
      IOUtils.copyBytes(System.in, out, 4096, false);
      out.finish();
    } finally {
      CodecPool.returnCompressor(compressor);
    }

MapReduce上使用压缩：

![](http://geleeq.github.io/blog/post_res/images/hadoop/mr-compress.jpg)

另一种方法在FileOutputFormat中使用方法设置

	Job job = new Job();
    job.setJarByClass(MaxTemperature.class);

    FileInputFormat.addInputPath(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));
    
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);
    
    FileOutputFormat.setCompressOutput(job, true); //很关键
    FileOutputFormat.setOutputCompressorClass(job, GzipCodec.class); //很关键

![](http://geleeq.github.io/blog/post_res/images/hadoop/map-compress.jpg)

另一种方法在Configuraion上设置

	Configuration conf = new Configuration();
    conf.setBoolean("mapred.compress.map.output", true);
    conf.setClass("mapred.map.output.compression.codec", GzipCodec.class,
        CompressionCodec.class);
    Job job = new Job(conf); //很关键
	...

### 序列化 ###

//TODO

### 序列化框架 ###

//TODO
